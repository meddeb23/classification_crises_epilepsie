{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kears_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWh7X6Uz38NlIs+Q7n4/rN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meddeb23/classification_crises_epilepsie/blob/main/kears_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BM9x-kknOmO",
        "outputId": "7ed794ca-d7f8-4016-c0d2-3d863860a38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "1.21.5\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Conv1D, MaxPooling1D,Flatten,BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "\n",
        "numpy.random.seed(2)\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = tensorflow.compat.v1.ConfigProto()\n",
        "sess = tensorflow.compat.v1.Session(config=config)"
      ],
      "metadata": {
        "id": "gDEbfudJneHl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwTiXMuXDva",
        "outputId": "31041640-ee96-4377-b679-9ec0b9171515"
      },
      "source": [
        "dataset_e1= pd.read_csv(\"./stand_norm_e1.txt\",delimiter=\" \",header=None)\n",
        "dataset_e1=numpy.array(dataset_e1,float)\n",
        "\n",
        "X_e1 = dataset_e1[:,:8]\n",
        "#m=np.max(X_e1)\n",
        "print(X_e1.shape)\n",
        "dataset_e2= pd.read_csv(\"./stand_norm_e2.txt\",delimiter=\" \",header=None)\n",
        "dataset_e2=numpy.array(dataset_e2,float)\n",
        "\n",
        "X_e2 = dataset_e2[:,:8]\n",
        "#n=np.max(X_e2)\n",
        "#X_e2=X_e2/n\n",
        "print(X_e2.shape)\n",
        "x_train_test=np.zeros((X_e1.shape[0],X_e1.shape[1],2))\n",
        "x_train_test[:,:,0]=X_e1\n",
        "x_train_test[:,:,1]=X_e2\n",
        "print(x_train_test.shape)\n",
        "#print(x_test.shape)\n",
        "dataset_output= pd.read_csv(\"./y2_e1.txt\",delimiter=\" \",header=None)\n",
        "dataset_output=numpy.array(dataset_output,float)\n",
        "dataset_output_new= dataset_output.reshape(dataset_output.shape[0], )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13762, 8)\n",
            "(13762, 8)\n",
            "(13762, 8, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train_test, dataset_output_new, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "-UixZGSToHpf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new=x_train\n",
        "x_test_new=x_test"
      ],
      "metadata": {
        "id": "mKEw9QzmoJvn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xvAiyJEYjYB",
        "outputId": "c5c18042-b645-4a30-e749-3153df53c281"
      },
      "source": [
        "print(x_train_new.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "Y_train_new = tensorflow.keras.utils.to_categorical( y_train)\n",
        "Y_test_new = tensorflow.keras.utils.to_categorical(y_test)\n",
        "print(Y_train_new.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11697, 8, 2)\n",
            "(11697,)\n",
            "(11697, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (x_train_new.shape)\n",
        "x_train_new=x_train_new.reshape(x_train_new.shape[0],  x_train_new.shape[1], 2)\n",
        "x_test_new = x_test_new.reshape(x_test_new.shape[0], x_test_new.shape[1],2 )\n",
        "print (x_train_new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4-AOfA3oJk3",
        "outputId": "b67a602a-630f-4ee1-ec4d-cbca233e4191"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11697, 8, 2)\n",
            "(11697, 8, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_new[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz9uOySktBth",
        "outputId": "ae9b0c06-c06a-4277-aa2a-e7c2eb1d6414"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[606.096047 279.596588]\n",
            " [  6.725553   5.081766]\n",
            " [ 13.935761  14.926257]\n",
            " [ 55.067419  17.357456]\n",
            " [173.842822  15.367078]\n",
            " [146.524492 196.86403 ]\n",
            " [  0.898929   0.898929]\n",
            " [  0.670072   1.078618]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=3,\n",
        "                 activation='relu',data_format=\"channels_last\",padding='same',\n",
        "                 input_shape=(8,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(64, 3, activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
        "model.add(Dropout(0.3))#0.1\n",
        "model.add(Conv1D(512, 3, activation='relu',padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2,strides=None))\n",
        "model.add(Dropout(0.4))#0.1\n",
        "model.add(Flatten()) # Flatten is the input layer of the Fully Connected\n",
        "model.add(Dense(100, activation='relu')) # gets input size from flatten\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))#0.2\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXluTR0CplkB",
        "outputId": "90e9c2b4-cd7b-4f96-ea2d-23077db86d1f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_12 (Conv1D)          (None, 8, 32)             224       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 8, 32)            128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_13 (Conv1D)          (None, 8, 64)             6208      \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 8, 64)            256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 4, 64)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 4, 64)             0         \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 4, 512)            98816     \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPooling  (None, 2, 512)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 2, 512)            0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               102500    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 208,835\n",
            "Trainable params: 208,443\n",
            "Non-trainable params: 392\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Activation , LSTM , Dropout , AveragePooling3D\n",
        "def model(alpha = 0.001):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(56, input_shape=(8,2), return_sequences=True))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(56))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(80))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(100))\n",
        "    # model.add(Activation('softmax'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
        "                  optimizer=tensorflow.keras.optimizers.Adam(learning_rate=alpha),\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjCxrLMn-xDU",
        "outputId": "ed5040a1-ab29-4ce2-a154-179d614026fb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 8, 56)             13216     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 8, 56)             0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 8, 56)            224       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 56)                25312     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 56)                0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 80)                4560      \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 80)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 100)               8100      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,715\n",
            "Trainable params: 51,603\n",
            "Non-trainable params: 112\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath='./'+'_best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
        "\n",
        "history =model.fit(x_train_new, Y_train_new, epochs=150, batch_size=512, verbose=1, validation_split=0.2, callbacks=[checkpointer])\n",
        "\n",
        "model.save('weights_cnn2.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPJI049Iprb4",
        "outputId": "0801245f-5771-44e0-a5ca-edd5728730ae"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.8195 - accuracy: 0.5920\n",
            "Epoch 1: val_loss improved from inf to 0.72844, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 6s 119ms/step - loss: 0.8170 - accuracy: 0.5922 - val_loss: 0.7284 - val_accuracy: 0.6321\n",
            "Epoch 2/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.6740 - accuracy: 0.6578\n",
            "Epoch 2: val_loss improved from 0.72844 to 0.70468, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.6731 - accuracy: 0.6588 - val_loss: 0.7047 - val_accuracy: 0.5863\n",
            "Epoch 3/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.6506 - accuracy: 0.6762\n",
            "Epoch 3: val_loss improved from 0.70468 to 0.67790, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.6498 - accuracy: 0.6769 - val_loss: 0.6779 - val_accuracy: 0.6355\n",
            "Epoch 4/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.6317 - accuracy: 0.6924\n",
            "Epoch 4: val_loss improved from 0.67790 to 0.66609, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.6314 - accuracy: 0.6921 - val_loss: 0.6661 - val_accuracy: 0.6457\n",
            "Epoch 5/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.6206 - accuracy: 0.7002\n",
            "Epoch 5: val_loss improved from 0.66609 to 0.64197, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.6199 - accuracy: 0.7005 - val_loss: 0.6420 - val_accuracy: 0.6803\n",
            "Epoch 6/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.6069 - accuracy: 0.7118\n",
            "Epoch 6: val_loss improved from 0.64197 to 0.63205, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.6060 - accuracy: 0.7125 - val_loss: 0.6320 - val_accuracy: 0.6996\n",
            "Epoch 7/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5923 - accuracy: 0.7232\n",
            "Epoch 7: val_loss improved from 0.63205 to 0.61540, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.5914 - accuracy: 0.7241 - val_loss: 0.6154 - val_accuracy: 0.6970\n",
            "Epoch 8/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5794 - accuracy: 0.7308\n",
            "Epoch 8: val_loss did not improve from 0.61540\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.5796 - accuracy: 0.7308 - val_loss: 0.6239 - val_accuracy: 0.6868\n",
            "Epoch 9/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7408\n",
            "Epoch 9: val_loss improved from 0.61540 to 0.59934, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.5687 - accuracy: 0.7406 - val_loss: 0.5993 - val_accuracy: 0.7094\n",
            "Epoch 10/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5544 - accuracy: 0.7503\n",
            "Epoch 10: val_loss improved from 0.59934 to 0.58894, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.5550 - accuracy: 0.7501 - val_loss: 0.5889 - val_accuracy: 0.7171\n",
            "Epoch 11/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5491 - accuracy: 0.7502\n",
            "Epoch 11: val_loss did not improve from 0.58894\n",
            "19/19 [==============================] - 2s 111ms/step - loss: 0.5495 - accuracy: 0.7500 - val_loss: 0.5990 - val_accuracy: 0.7047\n",
            "Epoch 12/150\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.7536\n",
            "Epoch 12: val_loss improved from 0.58894 to 0.57061, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 2s 132ms/step - loss: 0.5411 - accuracy: 0.7536 - val_loss: 0.5706 - val_accuracy: 0.7244\n",
            "Epoch 13/150\n",
            "19/19 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.7552\n",
            "Epoch 13: val_loss improved from 0.57061 to 0.56969, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 2s 121ms/step - loss: 0.5308 - accuracy: 0.7552 - val_loss: 0.5697 - val_accuracy: 0.7248\n",
            "Epoch 14/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5231 - accuracy: 0.7602\n",
            "Epoch 14: val_loss improved from 0.56969 to 0.55497, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 0.5229 - accuracy: 0.7606 - val_loss: 0.5550 - val_accuracy: 0.7393\n",
            "Epoch 15/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5144 - accuracy: 0.7658\n",
            "Epoch 15: val_loss did not improve from 0.55497\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.5148 - accuracy: 0.7650 - val_loss: 0.5750 - val_accuracy: 0.7265\n",
            "Epoch 16/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5105 - accuracy: 0.7641\n",
            "Epoch 16: val_loss did not improve from 0.55497\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.5106 - accuracy: 0.7642 - val_loss: 0.5766 - val_accuracy: 0.7321\n",
            "Epoch 17/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.5098 - accuracy: 0.7682\n",
            "Epoch 17: val_loss improved from 0.55497 to 0.53669, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.5095 - accuracy: 0.7685 - val_loss: 0.5367 - val_accuracy: 0.7462\n",
            "Epoch 18/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4977 - accuracy: 0.7721\n",
            "Epoch 18: val_loss did not improve from 0.53669\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4965 - accuracy: 0.7734 - val_loss: 0.5527 - val_accuracy: 0.7444\n",
            "Epoch 19/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4941 - accuracy: 0.7745\n",
            "Epoch 19: val_loss did not improve from 0.53669\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4931 - accuracy: 0.7755 - val_loss: 0.5633 - val_accuracy: 0.7363\n",
            "Epoch 20/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4896 - accuracy: 0.7780\n",
            "Epoch 20: val_loss did not improve from 0.53669\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4886 - accuracy: 0.7783 - val_loss: 0.5471 - val_accuracy: 0.7457\n",
            "Epoch 21/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4835 - accuracy: 0.7792\n",
            "Epoch 21: val_loss improved from 0.53669 to 0.52481, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.4829 - accuracy: 0.7797 - val_loss: 0.5248 - val_accuracy: 0.7538\n",
            "Epoch 22/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4816 - accuracy: 0.7804\n",
            "Epoch 22: val_loss improved from 0.52481 to 0.50773, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4820 - accuracy: 0.7803 - val_loss: 0.5077 - val_accuracy: 0.7556\n",
            "Epoch 23/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4737 - accuracy: 0.7865\n",
            "Epoch 23: val_loss did not improve from 0.50773\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.4740 - accuracy: 0.7864 - val_loss: 0.5177 - val_accuracy: 0.7538\n",
            "Epoch 24/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4715 - accuracy: 0.7891\n",
            "Epoch 24: val_loss improved from 0.50773 to 0.48562, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4727 - accuracy: 0.7887 - val_loss: 0.4856 - val_accuracy: 0.7722\n",
            "Epoch 25/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4661 - accuracy: 0.7886\n",
            "Epoch 25: val_loss improved from 0.48562 to 0.48270, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 77ms/step - loss: 0.4654 - accuracy: 0.7888 - val_loss: 0.4827 - val_accuracy: 0.7752\n",
            "Epoch 26/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4601 - accuracy: 0.7909\n",
            "Epoch 26: val_loss improved from 0.48270 to 0.45869, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4610 - accuracy: 0.7906 - val_loss: 0.4587 - val_accuracy: 0.7970\n",
            "Epoch 27/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4595 - accuracy: 0.7914\n",
            "Epoch 27: val_loss did not improve from 0.45869\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4586 - accuracy: 0.7919 - val_loss: 0.4648 - val_accuracy: 0.7910\n",
            "Epoch 28/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4614 - accuracy: 0.7973\n",
            "Epoch 28: val_loss improved from 0.45869 to 0.45669, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.4609 - accuracy: 0.7974 - val_loss: 0.4567 - val_accuracy: 0.7974\n",
            "Epoch 29/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4588 - accuracy: 0.7923\n",
            "Epoch 29: val_loss did not improve from 0.45669\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.4581 - accuracy: 0.7925 - val_loss: 0.4716 - val_accuracy: 0.7833\n",
            "Epoch 30/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4569 - accuracy: 0.7938\n",
            "Epoch 30: val_loss improved from 0.45669 to 0.45165, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4566 - accuracy: 0.7941 - val_loss: 0.4517 - val_accuracy: 0.7996\n",
            "Epoch 31/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4494 - accuracy: 0.7993\n",
            "Epoch 31: val_loss did not improve from 0.45165\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4503 - accuracy: 0.7989 - val_loss: 0.4538 - val_accuracy: 0.7919\n",
            "Epoch 32/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4460 - accuracy: 0.7996\n",
            "Epoch 32: val_loss improved from 0.45165 to 0.43902, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4476 - accuracy: 0.7995 - val_loss: 0.4390 - val_accuracy: 0.8090\n",
            "Epoch 33/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4460 - accuracy: 0.7976\n",
            "Epoch 33: val_loss did not improve from 0.43902\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.4456 - accuracy: 0.7978 - val_loss: 0.4427 - val_accuracy: 0.8073\n",
            "Epoch 34/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4399 - accuracy: 0.8022\n",
            "Epoch 34: val_loss did not improve from 0.43902\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4398 - accuracy: 0.8022 - val_loss: 0.4400 - val_accuracy: 0.8047\n",
            "Epoch 35/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4362 - accuracy: 0.8026\n",
            "Epoch 35: val_loss did not improve from 0.43902\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4380 - accuracy: 0.8019 - val_loss: 0.4451 - val_accuracy: 0.8030\n",
            "Epoch 36/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4349 - accuracy: 0.8049\n",
            "Epoch 36: val_loss improved from 0.43902 to 0.42057, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4349 - accuracy: 0.8053 - val_loss: 0.4206 - val_accuracy: 0.8154\n",
            "Epoch 37/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4291 - accuracy: 0.8091\n",
            "Epoch 37: val_loss improved from 0.42057 to 0.42038, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.4291 - accuracy: 0.8088 - val_loss: 0.4204 - val_accuracy: 0.8124\n",
            "Epoch 38/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4247 - accuracy: 0.8105\n",
            "Epoch 38: val_loss improved from 0.42038 to 0.41640, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.4257 - accuracy: 0.8100 - val_loss: 0.4164 - val_accuracy: 0.8201\n",
            "Epoch 39/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4293 - accuracy: 0.8069\n",
            "Epoch 39: val_loss improved from 0.41640 to 0.41145, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4305 - accuracy: 0.8066 - val_loss: 0.4114 - val_accuracy: 0.8184\n",
            "Epoch 40/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4304 - accuracy: 0.8060\n",
            "Epoch 40: val_loss did not improve from 0.41145\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4294 - accuracy: 0.8063 - val_loss: 0.4232 - val_accuracy: 0.8103\n",
            "Epoch 41/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4195 - accuracy: 0.8118\n",
            "Epoch 41: val_loss did not improve from 0.41145\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4194 - accuracy: 0.8117 - val_loss: 0.4128 - val_accuracy: 0.8179\n",
            "Epoch 42/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4249 - accuracy: 0.8088\n",
            "Epoch 42: val_loss did not improve from 0.41145\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4251 - accuracy: 0.8082 - val_loss: 0.4127 - val_accuracy: 0.8218\n",
            "Epoch 43/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4202 - accuracy: 0.8168\n",
            "Epoch 43: val_loss did not improve from 0.41145\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4202 - accuracy: 0.8165 - val_loss: 0.4135 - val_accuracy: 0.8175\n",
            "Epoch 44/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4235 - accuracy: 0.8103\n",
            "Epoch 44: val_loss improved from 0.41145 to 0.40419, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.4234 - accuracy: 0.8106 - val_loss: 0.4042 - val_accuracy: 0.8248\n",
            "Epoch 45/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4235 - accuracy: 0.8139\n",
            "Epoch 45: val_loss did not improve from 0.40419\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4226 - accuracy: 0.8146 - val_loss: 0.4110 - val_accuracy: 0.8201\n",
            "Epoch 46/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4175 - accuracy: 0.8181\n",
            "Epoch 46: val_loss did not improve from 0.40419\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4164 - accuracy: 0.8191 - val_loss: 0.4151 - val_accuracy: 0.8162\n",
            "Epoch 47/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4124 - accuracy: 0.8212\n",
            "Epoch 47: val_loss did not improve from 0.40419\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4122 - accuracy: 0.8213 - val_loss: 0.4135 - val_accuracy: 0.8171\n",
            "Epoch 48/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4111 - accuracy: 0.8176\n",
            "Epoch 48: val_loss did not improve from 0.40419\n",
            "19/19 [==============================] - 1s 69ms/step - loss: 0.4098 - accuracy: 0.8183 - val_loss: 0.4111 - val_accuracy: 0.8239\n",
            "Epoch 49/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4025 - accuracy: 0.8223\n",
            "Epoch 49: val_loss improved from 0.40419 to 0.39290, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.4030 - accuracy: 0.8223 - val_loss: 0.3929 - val_accuracy: 0.8303\n",
            "Epoch 50/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4013 - accuracy: 0.8262\n",
            "Epoch 50: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.4016 - accuracy: 0.8256 - val_loss: 0.4082 - val_accuracy: 0.8214\n",
            "Epoch 51/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4067 - accuracy: 0.8203\n",
            "Epoch 51: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4084 - accuracy: 0.8190 - val_loss: 0.3995 - val_accuracy: 0.8252\n",
            "Epoch 52/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4022 - accuracy: 0.8236\n",
            "Epoch 52: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.4019 - accuracy: 0.8236 - val_loss: 0.3954 - val_accuracy: 0.8321\n",
            "Epoch 53/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3899 - accuracy: 0.8285\n",
            "Epoch 53: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3907 - accuracy: 0.8276 - val_loss: 0.4219 - val_accuracy: 0.8209\n",
            "Epoch 54/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4058 - accuracy: 0.8179\n",
            "Epoch 54: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.4060 - accuracy: 0.8179 - val_loss: 0.4031 - val_accuracy: 0.8201\n",
            "Epoch 55/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.4026 - accuracy: 0.8216\n",
            "Epoch 55: val_loss did not improve from 0.39290\n",
            "19/19 [==============================] - 1s 68ms/step - loss: 0.4021 - accuracy: 0.8217 - val_loss: 0.3936 - val_accuracy: 0.8235\n",
            "Epoch 56/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3895 - accuracy: 0.8301\n",
            "Epoch 56: val_loss improved from 0.39290 to 0.39224, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3902 - accuracy: 0.8299 - val_loss: 0.3922 - val_accuracy: 0.8282\n",
            "Epoch 57/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3923 - accuracy: 0.8288\n",
            "Epoch 57: val_loss did not improve from 0.39224\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3931 - accuracy: 0.8286 - val_loss: 0.3935 - val_accuracy: 0.8231\n",
            "Epoch 58/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3909 - accuracy: 0.8314\n",
            "Epoch 58: val_loss improved from 0.39224 to 0.38422, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3912 - accuracy: 0.8312 - val_loss: 0.3842 - val_accuracy: 0.8321\n",
            "Epoch 59/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3856 - accuracy: 0.8343\n",
            "Epoch 59: val_loss did not improve from 0.38422\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3861 - accuracy: 0.8341 - val_loss: 0.3847 - val_accuracy: 0.8321\n",
            "Epoch 60/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3833 - accuracy: 0.8341\n",
            "Epoch 60: val_loss did not improve from 0.38422\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3821 - accuracy: 0.8347 - val_loss: 0.3844 - val_accuracy: 0.8342\n",
            "Epoch 61/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3857 - accuracy: 0.8318\n",
            "Epoch 61: val_loss did not improve from 0.38422\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3849 - accuracy: 0.8321 - val_loss: 0.3893 - val_accuracy: 0.8312\n",
            "Epoch 62/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3873 - accuracy: 0.8313\n",
            "Epoch 62: val_loss improved from 0.38422 to 0.37839, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3875 - accuracy: 0.8314 - val_loss: 0.3784 - val_accuracy: 0.8325\n",
            "Epoch 63/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3835 - accuracy: 0.8298\n",
            "Epoch 63: val_loss improved from 0.37839 to 0.37440, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 76ms/step - loss: 0.3833 - accuracy: 0.8300 - val_loss: 0.3744 - val_accuracy: 0.8397\n",
            "Epoch 64/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3777 - accuracy: 0.8338\n",
            "Epoch 64: val_loss did not improve from 0.37440\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3771 - accuracy: 0.8342 - val_loss: 0.3835 - val_accuracy: 0.8308\n",
            "Epoch 65/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3788 - accuracy: 0.8341\n",
            "Epoch 65: val_loss did not improve from 0.37440\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3791 - accuracy: 0.8337 - val_loss: 0.3751 - val_accuracy: 0.8359\n",
            "Epoch 66/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3780 - accuracy: 0.8316\n",
            "Epoch 66: val_loss improved from 0.37440 to 0.37157, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3776 - accuracy: 0.8316 - val_loss: 0.3716 - val_accuracy: 0.8423\n",
            "Epoch 67/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3777 - accuracy: 0.8337\n",
            "Epoch 67: val_loss did not improve from 0.37157\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3776 - accuracy: 0.8339 - val_loss: 0.3767 - val_accuracy: 0.8278\n",
            "Epoch 68/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3756 - accuracy: 0.8345\n",
            "Epoch 68: val_loss improved from 0.37157 to 0.36664, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3777 - accuracy: 0.8332 - val_loss: 0.3666 - val_accuracy: 0.8397\n",
            "Epoch 69/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3808 - accuracy: 0.8331\n",
            "Epoch 69: val_loss did not improve from 0.36664\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3808 - accuracy: 0.8334 - val_loss: 0.3695 - val_accuracy: 0.8350\n",
            "Epoch 70/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3721 - accuracy: 0.8368\n",
            "Epoch 70: val_loss improved from 0.36664 to 0.35894, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3725 - accuracy: 0.8370 - val_loss: 0.3589 - val_accuracy: 0.8474\n",
            "Epoch 71/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3688 - accuracy: 0.8384\n",
            "Epoch 71: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3689 - accuracy: 0.8378 - val_loss: 0.3725 - val_accuracy: 0.8303\n",
            "Epoch 72/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3713 - accuracy: 0.8389\n",
            "Epoch 72: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3716 - accuracy: 0.8385 - val_loss: 0.3623 - val_accuracy: 0.8419\n",
            "Epoch 73/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3613 - accuracy: 0.8427\n",
            "Epoch 73: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3622 - accuracy: 0.8426 - val_loss: 0.3686 - val_accuracy: 0.8397\n",
            "Epoch 74/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3685 - accuracy: 0.8401\n",
            "Epoch 74: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3690 - accuracy: 0.8398 - val_loss: 0.3886 - val_accuracy: 0.8303\n",
            "Epoch 75/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3687 - accuracy: 0.8372\n",
            "Epoch 75: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3681 - accuracy: 0.8376 - val_loss: 0.3715 - val_accuracy: 0.8393\n",
            "Epoch 76/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3754 - accuracy: 0.8349\n",
            "Epoch 76: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3741 - accuracy: 0.8353 - val_loss: 0.3625 - val_accuracy: 0.8406\n",
            "Epoch 77/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3710 - accuracy: 0.8389\n",
            "Epoch 77: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3711 - accuracy: 0.8391 - val_loss: 0.3592 - val_accuracy: 0.8436\n",
            "Epoch 78/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3681 - accuracy: 0.8340\n",
            "Epoch 78: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3681 - accuracy: 0.8342 - val_loss: 0.3701 - val_accuracy: 0.8415\n",
            "Epoch 79/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3610 - accuracy: 0.8447\n",
            "Epoch 79: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3612 - accuracy: 0.8449 - val_loss: 0.3683 - val_accuracy: 0.8393\n",
            "Epoch 80/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3530 - accuracy: 0.8453\n",
            "Epoch 80: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3545 - accuracy: 0.8447 - val_loss: 0.3618 - val_accuracy: 0.8444\n",
            "Epoch 81/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3564 - accuracy: 0.8429\n",
            "Epoch 81: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3566 - accuracy: 0.8430 - val_loss: 0.3603 - val_accuracy: 0.8449\n",
            "Epoch 82/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3600 - accuracy: 0.8380\n",
            "Epoch 82: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3599 - accuracy: 0.8384 - val_loss: 0.3671 - val_accuracy: 0.8393\n",
            "Epoch 83/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3663 - accuracy: 0.8408\n",
            "Epoch 83: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3667 - accuracy: 0.8408 - val_loss: 0.3609 - val_accuracy: 0.8440\n",
            "Epoch 84/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3644 - accuracy: 0.8440\n",
            "Epoch 84: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3649 - accuracy: 0.8436 - val_loss: 0.3608 - val_accuracy: 0.8393\n",
            "Epoch 85/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3569 - accuracy: 0.8439\n",
            "Epoch 85: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3572 - accuracy: 0.8435 - val_loss: 0.3783 - val_accuracy: 0.8329\n",
            "Epoch 86/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3673 - accuracy: 0.8394\n",
            "Epoch 86: val_loss did not improve from 0.35894\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3674 - accuracy: 0.8395 - val_loss: 0.3659 - val_accuracy: 0.8406\n",
            "Epoch 87/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3576 - accuracy: 0.8454\n",
            "Epoch 87: val_loss improved from 0.35894 to 0.35749, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3587 - accuracy: 0.8444 - val_loss: 0.3575 - val_accuracy: 0.8462\n",
            "Epoch 88/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3532 - accuracy: 0.8445\n",
            "Epoch 88: val_loss did not improve from 0.35749\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3524 - accuracy: 0.8447 - val_loss: 0.3587 - val_accuracy: 0.8440\n",
            "Epoch 89/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3569 - accuracy: 0.8429\n",
            "Epoch 89: val_loss improved from 0.35749 to 0.35287, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3581 - accuracy: 0.8417 - val_loss: 0.3529 - val_accuracy: 0.8466\n",
            "Epoch 90/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3503 - accuracy: 0.8447\n",
            "Epoch 90: val_loss improved from 0.35287 to 0.34792, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3496 - accuracy: 0.8448 - val_loss: 0.3479 - val_accuracy: 0.8483\n",
            "Epoch 91/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3523 - accuracy: 0.8478\n",
            "Epoch 91: val_loss did not improve from 0.34792\n",
            "19/19 [==============================] - 1s 70ms/step - loss: 0.3521 - accuracy: 0.8477 - val_loss: 0.3530 - val_accuracy: 0.8457\n",
            "Epoch 92/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3461 - accuracy: 0.8521\n",
            "Epoch 92: val_loss improved from 0.34792 to 0.34706, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3465 - accuracy: 0.8519 - val_loss: 0.3471 - val_accuracy: 0.8474\n",
            "Epoch 93/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3419 - accuracy: 0.8498\n",
            "Epoch 93: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3426 - accuracy: 0.8496 - val_loss: 0.3608 - val_accuracy: 0.8496\n",
            "Epoch 94/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3456 - accuracy: 0.8484\n",
            "Epoch 94: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3458 - accuracy: 0.8486 - val_loss: 0.3573 - val_accuracy: 0.8517\n",
            "Epoch 95/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3385 - accuracy: 0.8526\n",
            "Epoch 95: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3386 - accuracy: 0.8525 - val_loss: 0.3639 - val_accuracy: 0.8432\n",
            "Epoch 96/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3441 - accuracy: 0.8488\n",
            "Epoch 96: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3444 - accuracy: 0.8490 - val_loss: 0.3618 - val_accuracy: 0.8457\n",
            "Epoch 97/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3416 - accuracy: 0.8526\n",
            "Epoch 97: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3409 - accuracy: 0.8529 - val_loss: 0.3590 - val_accuracy: 0.8436\n",
            "Epoch 98/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3395 - accuracy: 0.8515\n",
            "Epoch 98: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3391 - accuracy: 0.8518 - val_loss: 0.3599 - val_accuracy: 0.8415\n",
            "Epoch 99/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3488 - accuracy: 0.8470\n",
            "Epoch 99: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3499 - accuracy: 0.8464 - val_loss: 0.3474 - val_accuracy: 0.8517\n",
            "Epoch 100/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3406 - accuracy: 0.8543\n",
            "Epoch 100: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3403 - accuracy: 0.8540 - val_loss: 0.3508 - val_accuracy: 0.8483\n",
            "Epoch 101/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3397 - accuracy: 0.8530\n",
            "Epoch 101: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3406 - accuracy: 0.8522 - val_loss: 0.3598 - val_accuracy: 0.8432\n",
            "Epoch 102/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3438 - accuracy: 0.8490\n",
            "Epoch 102: val_loss did not improve from 0.34706\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3430 - accuracy: 0.8495 - val_loss: 0.3473 - val_accuracy: 0.8509\n",
            "Epoch 103/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3376 - accuracy: 0.8533\n",
            "Epoch 103: val_loss improved from 0.34706 to 0.34116, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3377 - accuracy: 0.8536 - val_loss: 0.3412 - val_accuracy: 0.8560\n",
            "Epoch 104/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3426 - accuracy: 0.8484\n",
            "Epoch 104: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3419 - accuracy: 0.8491 - val_loss: 0.3505 - val_accuracy: 0.8534\n",
            "Epoch 105/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3341 - accuracy: 0.8559\n",
            "Epoch 105: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3337 - accuracy: 0.8559 - val_loss: 0.3535 - val_accuracy: 0.8556\n",
            "Epoch 106/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3365 - accuracy: 0.8520\n",
            "Epoch 106: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3357 - accuracy: 0.8526 - val_loss: 0.3530 - val_accuracy: 0.8517\n",
            "Epoch 107/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3364 - accuracy: 0.8530\n",
            "Epoch 107: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3356 - accuracy: 0.8536 - val_loss: 0.3497 - val_accuracy: 0.8500\n",
            "Epoch 108/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3294 - accuracy: 0.8555\n",
            "Epoch 108: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3295 - accuracy: 0.8555 - val_loss: 0.3558 - val_accuracy: 0.8500\n",
            "Epoch 109/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3272 - accuracy: 0.8557\n",
            "Epoch 109: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3273 - accuracy: 0.8560 - val_loss: 0.3658 - val_accuracy: 0.8453\n",
            "Epoch 110/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3365 - accuracy: 0.8537\n",
            "Epoch 110: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3377 - accuracy: 0.8532 - val_loss: 0.3478 - val_accuracy: 0.8521\n",
            "Epoch 111/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3367 - accuracy: 0.8502\n",
            "Epoch 111: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3371 - accuracy: 0.8498 - val_loss: 0.3505 - val_accuracy: 0.8556\n",
            "Epoch 112/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3348 - accuracy: 0.8548\n",
            "Epoch 112: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3338 - accuracy: 0.8551 - val_loss: 0.3470 - val_accuracy: 0.8538\n",
            "Epoch 113/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3272 - accuracy: 0.8567\n",
            "Epoch 113: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3280 - accuracy: 0.8563 - val_loss: 0.3469 - val_accuracy: 0.8577\n",
            "Epoch 114/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3334 - accuracy: 0.8572\n",
            "Epoch 114: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3343 - accuracy: 0.8566 - val_loss: 0.3487 - val_accuracy: 0.8521\n",
            "Epoch 115/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3325 - accuracy: 0.8555\n",
            "Epoch 115: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3330 - accuracy: 0.8549 - val_loss: 0.3507 - val_accuracy: 0.8534\n",
            "Epoch 116/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3348 - accuracy: 0.8538\n",
            "Epoch 116: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3339 - accuracy: 0.8544 - val_loss: 0.3530 - val_accuracy: 0.8547\n",
            "Epoch 117/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3302 - accuracy: 0.8580\n",
            "Epoch 117: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3308 - accuracy: 0.8575 - val_loss: 0.3442 - val_accuracy: 0.8560\n",
            "Epoch 118/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3236 - accuracy: 0.8588\n",
            "Epoch 118: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3234 - accuracy: 0.8588 - val_loss: 0.3447 - val_accuracy: 0.8521\n",
            "Epoch 119/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3183 - accuracy: 0.8650\n",
            "Epoch 119: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3174 - accuracy: 0.8656 - val_loss: 0.3527 - val_accuracy: 0.8538\n",
            "Epoch 120/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3198 - accuracy: 0.8619\n",
            "Epoch 120: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.3207 - accuracy: 0.8617 - val_loss: 0.3657 - val_accuracy: 0.8470\n",
            "Epoch 121/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3196 - accuracy: 0.8608\n",
            "Epoch 121: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3193 - accuracy: 0.8614 - val_loss: 0.3423 - val_accuracy: 0.8594\n",
            "Epoch 122/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3197 - accuracy: 0.8612\n",
            "Epoch 122: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3209 - accuracy: 0.8610 - val_loss: 0.3431 - val_accuracy: 0.8637\n",
            "Epoch 123/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3156 - accuracy: 0.8606\n",
            "Epoch 123: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3156 - accuracy: 0.8607 - val_loss: 0.3465 - val_accuracy: 0.8594\n",
            "Epoch 124/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3197 - accuracy: 0.8632\n",
            "Epoch 124: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3193 - accuracy: 0.8635 - val_loss: 0.3486 - val_accuracy: 0.8603\n",
            "Epoch 125/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3206 - accuracy: 0.8604\n",
            "Epoch 125: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3223 - accuracy: 0.8596 - val_loss: 0.3615 - val_accuracy: 0.8598\n",
            "Epoch 126/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3215 - accuracy: 0.8640\n",
            "Epoch 126: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3220 - accuracy: 0.8640 - val_loss: 0.3515 - val_accuracy: 0.8543\n",
            "Epoch 127/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3257 - accuracy: 0.8575\n",
            "Epoch 127: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3262 - accuracy: 0.8575 - val_loss: 0.3418 - val_accuracy: 0.8603\n",
            "Epoch 128/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3144 - accuracy: 0.8596\n",
            "Epoch 128: val_loss did not improve from 0.34116\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3134 - accuracy: 0.8603 - val_loss: 0.3466 - val_accuracy: 0.8543\n",
            "Epoch 129/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3210 - accuracy: 0.8595\n",
            "Epoch 129: val_loss improved from 0.34116 to 0.33898, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3211 - accuracy: 0.8597 - val_loss: 0.3390 - val_accuracy: 0.8603\n",
            "Epoch 130/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3227 - accuracy: 0.8592\n",
            "Epoch 130: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3224 - accuracy: 0.8595 - val_loss: 0.3509 - val_accuracy: 0.8496\n",
            "Epoch 131/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3146 - accuracy: 0.8602\n",
            "Epoch 131: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3144 - accuracy: 0.8603 - val_loss: 0.3634 - val_accuracy: 0.8547\n",
            "Epoch 132/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3060 - accuracy: 0.8678\n",
            "Epoch 132: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3072 - accuracy: 0.8673 - val_loss: 0.3473 - val_accuracy: 0.8603\n",
            "Epoch 133/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3157 - accuracy: 0.8624\n",
            "Epoch 133: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.3146 - accuracy: 0.8630 - val_loss: 0.3463 - val_accuracy: 0.8603\n",
            "Epoch 134/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3101 - accuracy: 0.8649\n",
            "Epoch 134: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3100 - accuracy: 0.8652 - val_loss: 0.3401 - val_accuracy: 0.8560\n",
            "Epoch 135/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3160 - accuracy: 0.8623\n",
            "Epoch 135: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3174 - accuracy: 0.8620 - val_loss: 0.3553 - val_accuracy: 0.8598\n",
            "Epoch 136/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3063 - accuracy: 0.8688\n",
            "Epoch 136: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3075 - accuracy: 0.8682 - val_loss: 0.3397 - val_accuracy: 0.8590\n",
            "Epoch 137/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3095 - accuracy: 0.8609\n",
            "Epoch 137: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3102 - accuracy: 0.8602 - val_loss: 0.3480 - val_accuracy: 0.8590\n",
            "Epoch 138/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3098 - accuracy: 0.8622\n",
            "Epoch 138: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3115 - accuracy: 0.8615 - val_loss: 0.3521 - val_accuracy: 0.8556\n",
            "Epoch 139/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3077 - accuracy: 0.8698\n",
            "Epoch 139: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3068 - accuracy: 0.8702 - val_loss: 0.3473 - val_accuracy: 0.8577\n",
            "Epoch 140/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3159 - accuracy: 0.8602\n",
            "Epoch 140: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3159 - accuracy: 0.8605 - val_loss: 0.3519 - val_accuracy: 0.8526\n",
            "Epoch 141/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3108 - accuracy: 0.8647\n",
            "Epoch 141: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3121 - accuracy: 0.8641 - val_loss: 0.3483 - val_accuracy: 0.8573\n",
            "Epoch 142/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3072 - accuracy: 0.8633\n",
            "Epoch 142: val_loss did not improve from 0.33898\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3067 - accuracy: 0.8635 - val_loss: 0.3419 - val_accuracy: 0.8594\n",
            "Epoch 143/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3051 - accuracy: 0.8652\n",
            "Epoch 143: val_loss improved from 0.33898 to 0.33660, saving model to ./_best_weights.h5\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3053 - accuracy: 0.8650 - val_loss: 0.3366 - val_accuracy: 0.8615\n",
            "Epoch 144/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.8641\n",
            "Epoch 144: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3110 - accuracy: 0.8643 - val_loss: 0.3421 - val_accuracy: 0.8598\n",
            "Epoch 145/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3117 - accuracy: 0.8637\n",
            "Epoch 145: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3124 - accuracy: 0.8637 - val_loss: 0.3436 - val_accuracy: 0.8560\n",
            "Epoch 146/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3157 - accuracy: 0.8617\n",
            "Epoch 146: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 75ms/step - loss: 0.3152 - accuracy: 0.8617 - val_loss: 0.3718 - val_accuracy: 0.8496\n",
            "Epoch 147/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3111 - accuracy: 0.8618\n",
            "Epoch 147: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3109 - accuracy: 0.8618 - val_loss: 0.3416 - val_accuracy: 0.8568\n",
            "Epoch 148/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3045 - accuracy: 0.8662\n",
            "Epoch 148: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 73ms/step - loss: 0.3053 - accuracy: 0.8666 - val_loss: 0.3476 - val_accuracy: 0.8577\n",
            "Epoch 149/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3014 - accuracy: 0.8664\n",
            "Epoch 149: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3013 - accuracy: 0.8665 - val_loss: 0.3507 - val_accuracy: 0.8615\n",
            "Epoch 150/150\n",
            "18/19 [===========================>..] - ETA: 0s - loss: 0.3036 - accuracy: 0.8645\n",
            "Epoch 150: val_loss did not improve from 0.33660\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.3025 - accuracy: 0.8647 - val_loss: 0.3432 - val_accuracy: 0.8598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test_new, Y_test_new, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr6EC24EpyE_",
        "outputId": "ab3a4091-0a5d-46bb-fda9-0078fe299f0e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65/65 [==============================] - 0s 4ms/step - loss: 0.3308 - accuracy: 0.8605\n",
            "Test loss: 0.3307717740535736\n",
            "Test accuracy: 0.8605327010154724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt # shorcut for a plot function\n",
        "plt.plot(history.history['accuracy']) # training accuracy\n",
        "plt.plot(history.history['val_accuracy']) # validation accuracy\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9SywH-93p0uP",
        "outputId": "9b601831-84ff-4855-cb97-36a2eb5d108b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZRwv7MR7p4zS",
        "outputId": "d3fc4a44-b064-425d-c625-f55d816f2abe"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hh3lutSZUWO",
        "outputId": "92a795b5-4c77-42cb-c72c-fea72824969a"
      },
      "source": [
        "predictions = model.predict(x_test_new, batch_size=32, verbose=2)\n",
        "print (\"predicted images size :\",predictions.shape)\n",
        "print(predictions)\n",
        "print(Y_test_new.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65/65 - 1s - 993ms/epoch - 15ms/step\n",
            "predicted images size : (2065, 3)\n",
            "[[9.9983835e-01 1.6002438e-04 1.5376880e-06]\n",
            " [3.0815166e-01 6.9155723e-01 2.9110076e-04]\n",
            " [9.8948157e-01 1.0512369e-02 6.0564839e-06]\n",
            " ...\n",
            " [9.9927586e-01 5.8464694e-04 1.3944696e-04]\n",
            " [1.2234360e-03 9.9877661e-01 5.2752016e-09]\n",
            " [9.9982125e-01 1.6360865e-04 1.5109559e-05]]\n",
            "(2065, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score\n",
        "threshold_confusion = 0.5\n",
        "print (\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
        "y_pred = np.empty((predictions.shape[0]))\n",
        "y_test = np.empty((predictions.shape[0]))\n",
        "for i in range(predictions.shape[0]):\n",
        "    \n",
        "    y_pred[i]=np.argmax(predictions[i])\n",
        "    y_test[i]=np.argmax(Y_test_new[i])\n",
        "#print(y_pred)\n",
        "    \n",
        "       \n",
        "confusion = confusion_matrix(y_test,  y_pred)\n",
        "print (confusion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdqC-op4p8Ge",
        "outputId": "81cdd4ef-accf-44de-c64a-b9d4140fdf8b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion matrix:  Custom threshold (for positive) of 0.5\n",
            "[[782 197   1]\n",
            " [118 936   2]\n",
            " [  5   7  17]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The accuracy score on this random test-set is  :\", accuracy_score(y_test,  y_pred) )\n",
        "recall=recall_score(y_test,  y_pred, average=None)\n",
        "av_recall=sum(recall)/3\n",
        "print(\"recall\",recall, \"the avreage recall is \",av_recall)\n",
        "precision=precision_score(y_test,  y_pred,average=None)\n",
        "av_precision=sum(precision)/3\n",
        "\n",
        "print(\"precision\",precision,\"the avreage precision is \",av_precision)\n",
        "\n",
        "F1_score=(2*av_precision*av_recall)/(av_precision+av_recall)\n",
        "print(\"F1_score\",F1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSgAxV-2p8EI",
        "outputId": "0f772cd7-5744-45bb-eb4c-7ee2a1cc9f4f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy score on this random test-set is  : 0.8401937046004843\n",
            "recall [0.79795918 0.88636364 0.5862069 ] the avreage recall is  0.7568432388629432\n",
            "precision [0.8640884  0.82105263 0.85      ] the avreage precision is  0.8450470097896675\n",
            "F1_score 0.7985167728171312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OebFtqSIp8BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "U1L7P0uAp7-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gVERnlInp75A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}